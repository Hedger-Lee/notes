# 环境搭建

## 一、JDK安装

1. 解压jdk压缩文件

2. 配置环境变量

   ```shell
   export JAVA_HOME=/home/jdk1.8.0_144
   export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
   
   export PATH=$JAVA_HOME/bin:$PATH
   ```

3. 检查是否安装成功

## 二、Hadoop安装

1. 解压Hadoop压缩文件

2. 配置环境变量

   ```shell
   export HADOOP_HOME=/home/hadoop-2.8.2
   export HADOOP_INSTALL=$HADOOP_HOME
   export HADOOP_MAPRED_HOME=$HADOOP_HOME
   export HADOOP_COMMON_HOME=$HADOOP_HOME
   export HADOOP_HDFS_HOME=$HADOOP_HOME
   export YARN_HOME=$HADOOP_HOME
   export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
   export CLASSPATH=$($HADOOP_HOME/bin/hadoop classpath):$CLASSPATH
   
   export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib:$HADOOP_COMMON_LIB_NATIVE_DIR"
   
   export PATH=$JAVA_HOME/bin:$HADOOP_HOME/sbin:$HADOOP_HOME/bin:$PATH
   ```

3. 修改hadoop_env.sh文件
   存放位置：`$HADOOP_HOME/etc/hadoop/hadoop-env.sh`

   ```
   修改JAVA_HOME为实际的信息
   ```

4. 修改Hadoop里面的配置文件
   **core-site.xml**

   ```shell
   <configuration>
           <property>
                   <name>fs.default.name</name>
                   <value>hdfs://localhost:9000</value>
           </property>
   </configuration>
   ```

   **hdfs-site.xml**

   ```
   <configuration>
           <property>
                   <name>dfs.replication</name>
                   <value>1</value>
           </property>
   </configuration>
   ```

   **mapred-site.xml**(需要从模板文件复制过来)

   ```
   <configuration>
           <property>
                   <name>mapreduce.framework.name</name>
                   <value>yarn</value>
           </property>
   </configuration>
   ```

   **yarn-site.xml**

   ```
   <configuration>
           <property>
                   <name>yarn.nodemanager.aux-services</name>
                   <value>mapreduce_shuffle</value>
           </property>
   </configuration>
   ```

5. 初始化Hadoop

   ```
   hdfs namenode -format
   ```

6. 启动hadoop服务

   ```
   $HADOOP_HOME/sbin/start-all.sh
   ```

7. 使用jps查看是否成功

   ```
   SecondaryNameNode
   Jps
   ResourceManager
   NameNode
   NodeManager
   DataNode
   ```

8. 关闭linux服务器的防火墙，便于远程访问

   ```
   systemctl stop firewalld
   ```

9. 在浏览器查看是否成功
   使用8088端口

10. 开启Hadoop平台的免密登录

    ```
    先安装OpenSSH服务：yum -y install openssh-server
    
    执行：ssh-keygen -t rsa 命令，后面全部直接回车就可以了
    
    实现本地登录免密码登录，将本地的公钥导入授权文件就可以，这样下次再启动hadoop的相关服务，就不用输入密码了
    cat /root/.ssh/id_rsa.pub >> /root/.ssh/authorized_keys
    ```

    ## 三、Hive安装
    
    1. 解压Hive压缩文件
    
    2. 配置环境变量
    
       ```
       export HIVE_HOME=/home/apache-hive-2.1.1-bin
       export HIVE_CONF_DIR=$HIVE_HOME/conf
       
       export PATH=$JAVA_HOME/bin:$HADOOP_HOME/sbin:$HADOOP_HOME/bin:$HIVE_HOME/bin:$PATH
       ```
    
    3. 从模板文件复制为hive-site.xml
    
       ```
       cp $HIVE_HOME/conf/hive-default.xml.template $HIVE_HOME/conf/hive-site.xml
       ```
    
    4. hadoop命令创建目录并赋予权限
    
       ```
       $HADOOP_HOME/bin/hadoop fs -mkdir -p /user/hive/warehouse
       $HADOOP_HOME/bin/hadoop fs -chmod 777 /user/hive/warehouse
       
       $HADOOP_HOME/bin/hadoop fs -mkdir -p /tmp/hive/
       $HADOOP_HOME/bin/hadoop fs -chmod 777 /tmp/hive/
       ```
    
    5. 安装mysql
    
       ```
       yum -y install wget
       
       wget http://repo.mysql.com/mysql-community-release-el7-5.noarch.rpm
       
       rpm -ivh mysql-community-release-el7-5.noarch.rpm
       
       yum -y install mysql-server
       
       service mysqld start
       
       service mysqld status
       
       mysql -h127.0.0.1 -uroot
       grant all privileges on *.* to root@'%' identified by '123456';
       create database hive default charset='utf8';
       ```
    
    6. 修改hive-site.xml
    
       修改数据库相关配置
    
       ```
       ConnectionUserName：MySQL数据库用户名
       ConnectionPassword：MySQL数据库密码
       ConnectionURL：jdbc:mysql://ip:3306/hive
       ConnectionDriverName：com.mysql.jdbc.Driver
       ```
    
       修改hive数据库临时目录
    
       ```
       hive.exec.local.scratchdir:hive安装目录/tmp
       hive.downloaded.resources.dir:hive安装目录/tmp/resources
       hive.server2.logging.operation.log.location:hive安装目录/tmp/${system:user.name}/operation_logs
       ```
    
    7. 复制mysql驱动jar包到hive安装文件夹得lib目录里面
    
    8. 将 hive 数据仓库需要的表格和结构初始化到 mysql 数据库中
    
       ```
       schematool -dbType mysql -initSchema
       ```
    
    9. 检查是否能够进入hive命令行
    
    10. 使用50070查看
    
    

## 四、安装sqoop

1. 解压sqoop压缩文件

2. 配置环境变量

   ```
   export SQOOP_HOME=/home/sqoop-1.4.5-cdh5.3.6
   
   export PATH=$JAVA_HOME/bin:$HADOOP_HOME/sbin:$HADOOP_HOME/bin:$HIVE_HOME/bin:$SQOOP_HOME/bin:$PATH
   ```

3. 复制模板文件为sqoop-env.sh

4. 修改配置文件内容

   ```
   export HADOOP_COMMON_HOME=
   export HADOOP_MAPRED_HOME=
   export HIVE_HOME=
   ```

5. 复制mysql驱动jar包到sqoop的lib文件夹

6. 复制oracle驱动jar包到sqoop的lib文件夹

7. 检查是否安装成功

   ```
   sqoop list-tables --connect jdbc:oracle:thin:@192.168.3.22:1521/ORCL --username hedger --password 123456
   
   sqoop list-tables --connect  jdbc:oracle:thin:@192.168.3.22:1521/ORCL  --username hedger  --password 123456
   ```

   

## 五、DBeaver连接

1. 修改Hadoop配置文件core-site.xml

   ```
   <property>
       <name>hadoop.proxyuser.root.hosts</name>
       <value>*</value>
   </property>
   <property>
       <name>hadoop.proxyuser.root.groups</name>
       <value>*</value>
   </property>
   ```

2. 使用DBeaver创建新连接，添加四个驱动文件

3. hive数据hiveserver2监听



问题：

DataNode启动不了

将/tmp/hadoop-root/dfs/

name中的clusterID去覆盖data中clusterID